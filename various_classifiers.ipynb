{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from new_datasets_py import create_subsets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (balanced_accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, log_loss)\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            close_values  label\n",
      "0      [0.695589, 0.742796, 0.86392, 0.734774, 1.07, ...      0\n",
      "1      [0.742796, 0.86392, 0.734774, 1.07, 1.43, 1.33...      1\n",
      "2      [0.86392, 0.734774, 1.07, 1.43, 1.33, 1.4, 1.4...      1\n",
      "3      [0.734774, 1.07, 1.43, 1.33, 1.4, 1.4, 1.31, 2...      1\n",
      "4      [1.07, 1.43, 1.33, 1.4, 1.4, 1.31, 2.38, 3.18,...      1\n",
      "...                                                  ...    ...\n",
      "21966  [0.125544, 0.122172, 0.119888, 0.117055, 0.100...      1\n",
      "21967  [0.122172, 0.119888, 0.117055, 0.100511, 0.112...      1\n",
      "21968  [0.119888, 0.117055, 0.100511, 0.112595, 0.111...      1\n",
      "21969  [0.117055, 0.100511, 0.112595, 0.11143, 0.1131...      1\n",
      "21970  [0.100511, 0.112595, 0.11143, 0.113184, 0.1123...      1\n",
      "\n",
      "[21971 rows x 2 columns]\n",
      "close_values    0\n",
      "label           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/um/lib/python3.11/site-packages/pandas/core/indexes/base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
      "  return Index(sequences[0], name=names)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('crypto-markets.csv')\n",
    "filtered_data = data[data['ranknow'] < 30]\n",
    "filtered_data.loc[:, 'date'] = pd.to_datetime(filtered_data['date'])\n",
    "filtered_data.set_index('date', inplace=True)\n",
    "datasets_with_labels = []\n",
    "\n",
    "grouped = filtered_data.groupby('slug')\n",
    "\n",
    "for crypto, group in grouped:\n",
    "    close_values = group['close'].values\n",
    "\n",
    "    for start in range(len(close_values) - 9):\n",
    "        end = start + 10\n",
    "        window = close_values[start:end]\n",
    "        value_day_7 = window[6]  \n",
    "        value_day_10 = window[9] \n",
    "        label = 1 if value_day_10 > value_day_7 else 0\n",
    "\n",
    "        datasets_with_labels.append((window, label))\n",
    "\n",
    "combined_table = pd.DataFrame(datasets_with_labels, columns=['close_values', 'label'])\n",
    "\n",
    "# Print to check\n",
    "print(combined_table)\n",
    "missing_values = combined_table.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking balance of the dataset\n",
    "print(combined_table['label'].value_counts())\n",
    "combined_table['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "class_counts = combined_table['label'].value_counts()\n",
    "imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "X = combined_table['close_values'].apply(lambda x: x[:7]).tolist()\n",
    "y = combined_table['label'].astype(int).tolist()\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Performance Metrics:\n",
      "Balanced_accuracy:\n",
      "   Multi-Layer Perceptron: Mean = 0.5175, Std = 0.0098\n",
      "   Decision Tree: Mean = 0.5498, Std = 0.0064\n",
      "   Random Forest: Mean = 0.5708, Std = 0.0100\n",
      "   Balanced Random Forest: Mean = 0.5688, Std = 0.0092\n",
      "   Logistic Regression: Mean = 0.5135, Std = 0.0067\n",
      "   Support Vector Machine: Mean = 0.5057, Std = 0.0027\n",
      "   K-Nearest Neighbors: Mean = 0.5652, Std = 0.0064\n",
      "\n",
      "Precision:\n",
      "   Multi-Layer Perceptron: Mean = 0.4787, Std = 0.1748\n",
      "   Decision Tree: Mean = 0.5202, Std = 0.0064\n",
      "   Random Forest: Mean = 0.5481, Std = 0.0111\n",
      "   Balanced Random Forest: Mean = 0.5388, Std = 0.0088\n",
      "   Logistic Regression: Mean = 0.5175, Std = 0.0254\n",
      "   Support Vector Machine: Mean = 0.5156, Std = 0.0329\n",
      "   K-Nearest Neighbors: Mean = 0.5395, Std = 0.0069\n",
      "\n",
      "Recall:\n",
      "   Multi-Layer Perceptron: Mean = 0.2691, Std = 0.1711\n",
      "   Decision Tree: Mean = 0.5200, Std = 0.0136\n",
      "   Random Forest: Mean = 0.5103, Std = 0.0180\n",
      "   Balanced Random Forest: Mean = 0.5502, Std = 0.0193\n",
      "   Logistic Regression: Mean = 0.1481, Std = 0.0077\n",
      "   Support Vector Machine: Mean = 0.0816, Std = 0.0332\n",
      "   K-Nearest Neighbors: Mean = 0.5173, Std = 0.0114\n",
      "\n",
      "F1:\n",
      "   Multi-Layer Perceptron: Mean = 0.3107, Std = 0.1783\n",
      "   Decision Tree: Mean = 0.5200, Std = 0.0093\n",
      "   Random Forest: Mean = 0.5284, Std = 0.0136\n",
      "   Balanced Random Forest: Mean = 0.5443, Std = 0.0130\n",
      "   Logistic Regression: Mean = 0.2303, Std = 0.0113\n",
      "   Support Vector Machine: Mean = 0.1375, Std = 0.0477\n",
      "   K-Nearest Neighbors: Mean = 0.5282, Std = 0.0084\n",
      "\n",
      "Roc_auc:\n",
      "   Multi-Layer Perceptron: Mean = 0.5348, Std = 0.0210\n",
      "   Decision Tree: Mean = 0.5499, Std = 0.0064\n",
      "   Random Forest: Mean = 0.6130, Std = 0.0107\n",
      "   Balanced Random Forest: Mean = 0.6118, Std = 0.0108\n",
      "   Logistic Regression: Mean = 0.5225, Std = 0.0091\n",
      "   Support Vector Machine: Mean = nan, Std = nan\n",
      "   K-Nearest Neighbors: Mean = 0.5943, Std = 0.0058\n",
      "\n",
      "Log_loss:\n",
      "   Multi-Layer Perceptron: Mean = 1.2110, Std = 0.2875\n",
      "   Decision Tree: Mean = 16.1494, Std = 0.2206\n",
      "   Random Forest: Mean = 0.6806, Std = 0.0074\n",
      "   Balanced Random Forest: Mean = 0.6789, Std = 0.0071\n",
      "   Logistic Regression: Mean = 0.6934, Std = 0.0005\n",
      "   Support Vector Machine: Mean = nan, Std = nan\n",
      "   K-Nearest Neighbors: Mean = 2.0887, Std = 0.0689\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/13/vypvgv0x0n13zkc7gg4wg7hr0000gn/T/ipykernel_65366/3569623526.py:39: RuntimeWarning: Mean of empty slice\n",
      "  mean_score = np.nanmean(scores[metric])\n",
      "/opt/anaconda3/envs/um/lib/python3.11/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "    \"Multi-Layer Perceptron\": MLPClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Balanced Random Forest\": BalancedRandomForestClassifier(replacement=True, sampling_strategy='all', random_state=42, bootstrap=False),\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "results = {clf_name: {'balanced_accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': [], 'log_loss': []} for clf_name in classifiers}\n",
    "\n",
    "# Evaluation loop\n",
    "for clf_name, clf in classifiers.items():\n",
    "    for train, test in rskf.split(X, y):\n",
    "        model = clf\n",
    "        model.fit(X[train], y[train])\n",
    "        y_pred = model.predict(X[test])\n",
    "        y_prob = model.predict_proba(X[test]) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        results[clf_name]['balanced_accuracy'].append(balanced_accuracy_score(y[test], y_pred))\n",
    "        results[clf_name]['precision'].append(precision_score(y[test], y_pred))\n",
    "        results[clf_name]['recall'].append(recall_score(y[test], y_pred))\n",
    "        results[clf_name]['f1'].append(f1_score(y[test], y_pred))\n",
    "        if y_prob is not None:\n",
    "            results[clf_name]['roc_auc'].append(roc_auc_score(y[test], y_prob[:, 1]))\n",
    "            results[clf_name]['log_loss'].append(log_loss(y[test], y_prob))\n",
    "        else:\n",
    "            # For classifiers without predict_proba, handle ROC AUC and Log Loss differently\n",
    "            results[clf_name]['roc_auc'].append(np.nan)\n",
    "            results[clf_name]['log_loss'].append(np.nan)\n",
    "\n",
    "\n",
    "print(\"Mean Performance Metrics:\")\n",
    "for metric in ['balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'log_loss']:\n",
    "    print(f\"{metric.capitalize()}:\")\n",
    "    for clf_name, scores in results.items():\n",
    "        mean_score = np.nanmean(scores[metric])  \n",
    "        std_score = np.nanstd(scores[metric])    \n",
    "        print(f\"   {clf_name}: Mean = {mean_score:.4f}, Std = {std_score:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "metrics = ['balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'log_loss']\n",
    "titles = ['Balanced Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Log Loss']\n",
    "\n",
    "for ax, metric, title in zip(axs.ravel(), metrics, titles):\n",
    "    ax.boxplot([results[clf][metric] for clf in classifiers], labels=classifiers.keys())\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Classifiers')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_xticklabels(classifiers.keys(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron Performance Metrics:\n",
      "   Balanced_accuracy: Mean = 0.5175, Std = 0.0098\n",
      "   Precision: Mean = 0.4787, Std = 0.1748\n",
      "   Recall: Mean = 0.2691, Std = 0.1711\n",
      "   F1: Mean = 0.3107, Std = 0.1783\n",
      "   Roc_auc: Mean = 0.5348, Std = 0.0210\n",
      "   Log_loss: Mean = 1.2110, Std = 0.2875\n",
      "Decision Tree Performance Metrics:\n",
      "   Balanced_accuracy: Mean = 0.5498, Std = 0.0064\n",
      "   Precision: Mean = 0.5202, Std = 0.0064\n",
      "   Recall: Mean = 0.5200, Std = 0.0136\n",
      "   F1: Mean = 0.5200, Std = 0.0093\n",
      "   Roc_auc: Mean = 0.5499, Std = 0.0064\n",
      "   Log_loss: Mean = 16.1494, Std = 0.2206\n",
      "Random Forest Performance Metrics:\n",
      "   Balanced_accuracy: Mean = 0.5708, Std = 0.0100\n",
      "   Precision: Mean = 0.5481, Std = 0.0111\n",
      "   Recall: Mean = 0.5103, Std = 0.0180\n",
      "   F1: Mean = 0.5284, Std = 0.0136\n",
      "   Roc_auc: Mean = 0.6130, Std = 0.0107\n",
      "   Log_loss: Mean = 0.6806, Std = 0.0074\n",
      "Balanced Random Forest Performance Metrics:\n",
      "   Balanced_accuracy: Mean = 0.5688, Std = 0.0092\n",
      "   Precision: Mean = 0.5388, Std = 0.0088\n",
      "   Recall: Mean = 0.5502, Std = 0.0193\n",
      "   F1: Mean = 0.5443, Std = 0.0130\n",
      "   Roc_auc: Mean = 0.6118, Std = 0.0108\n",
      "   Log_loss: Mean = 0.6789, Std = 0.0071\n",
      "Logistic Regression Performance Metrics:\n",
      "   Balanced_accuracy: Mean = 0.5135, Std = 0.0067\n",
      "   Precision: Mean = 0.5175, Std = 0.0254\n",
      "   Recall: Mean = 0.1481, Std = 0.0077\n",
      "   F1: Mean = 0.2303, Std = 0.0113\n",
      "   Roc_auc: Mean = 0.5225, Std = 0.0091\n",
      "   Log_loss: Mean = 0.6934, Std = 0.0005\n",
      "Support Vector Machine Performance Metrics:\n",
      "   Balanced_accuracy: Mean = 0.5057, Std = 0.0027\n",
      "   Precision: Mean = 0.5156, Std = 0.0329\n",
      "   Recall: Mean = 0.0816, Std = 0.0332\n",
      "   F1: Mean = 0.1375, Std = 0.0477\n",
      "   Roc_auc: Mean = nan, Std = nan\n",
      "   Log_loss: Mean = nan, Std = nan\n",
      "K-Nearest Neighbors Performance Metrics:\n",
      "   Balanced_accuracy: Mean = 0.5652, Std = 0.0064\n",
      "   Precision: Mean = 0.5395, Std = 0.0069\n",
      "   Recall: Mean = 0.5173, Std = 0.0114\n",
      "   F1: Mean = 0.5282, Std = 0.0084\n",
      "   Roc_auc: Mean = 0.5943, Std = 0.0058\n",
      "   Log_loss: Mean = 2.0887, Std = 0.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/13/vypvgv0x0n13zkc7gg4wg7hr0000gn/T/ipykernel_65366/2920815958.py:5: RuntimeWarning: Mean of empty slice\n",
      "  mean_score = np.nanmean(scores)  # Use nanmean to handle NaN values for some metrics\n",
      "/opt/anaconda3/envs/um/lib/python3.11/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJądro Kernel uległo awarii podczas wykonywania kodu w bieżącej komórce lub w poprzedniej komórce. \n",
      "\u001b[1;31mPrzejrzyj kod w komórkach, aby zidentyfikować możliwą przyczynę awarii. \n",
      "\u001b[1;31mKliknij <a href='https://aka.ms/vscodeJupyterKernelCrash'>tutaj</a>, aby uzyskać więcej informacji. \n",
      "\u001b[1;31mAby uzyskać dalsze szczegóły, wyświetl <a href='command:jupyter.viewOutput'>dziennik</a> Jupyter."
     ]
    }
   ],
   "source": [
    "# Summary of results\n",
    "for clf_name, metrics in results.items():\n",
    "    print(f\"{clf_name} Performance Metrics:\")\n",
    "    for metric, scores in metrics.items():\n",
    "        mean_score = np.nanmean(scores)  # Use nanmean to handle NaN values for some metrics\n",
    "        std_score = np.nanstd(scores)    # Use nanstd to handle NaN values for some metrics\n",
    "        print(f\"   {metric.capitalize()}: Mean = {mean_score:.4f}, Std = {std_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
