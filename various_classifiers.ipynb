{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from new_datasets_py import create_subsets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import (balanced_accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, log_loss)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            close_values  label\n",
      "0      [0.695589, 0.742796, 0.86392, 0.734774, 1.07, ...      0\n",
      "1      [0.742796, 0.86392, 0.734774, 1.07, 1.43, 1.33...      1\n",
      "2      [0.86392, 0.734774, 1.07, 1.43, 1.33, 1.4, 1.4...      1\n",
      "3      [0.734774, 1.07, 1.43, 1.33, 1.4, 1.4, 1.31, 2...      1\n",
      "4      [1.07, 1.43, 1.33, 1.4, 1.4, 1.31, 2.38, 3.18,...      1\n",
      "...                                                  ...    ...\n",
      "21966  [0.125544, 0.122172, 0.119888, 0.117055, 0.100...      1\n",
      "21967  [0.122172, 0.119888, 0.117055, 0.100511, 0.112...      1\n",
      "21968  [0.119888, 0.117055, 0.100511, 0.112595, 0.111...      1\n",
      "21969  [0.117055, 0.100511, 0.112595, 0.11143, 0.1131...      1\n",
      "21970  [0.100511, 0.112595, 0.11143, 0.113184, 0.1123...      1\n",
      "\n",
      "[21971 rows x 2 columns]\n",
      "close_values    0\n",
      "label           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\time-stamps-predicition\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7588: FutureWarning: Dtype inference on a pandas object (Series, Index, ExtensionArray) is deprecated. The Index constructor will keep the original dtype in the future. Call `infer_objects` on the result to get the old behavior.\n",
      "  return Index(sequences[0], name=names)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('crypto-markets.csv')\n",
    "filtered_data = data[data['ranknow'] < 30]\n",
    "filtered_data.loc[:, 'date'] = pd.to_datetime(filtered_data['date'])\n",
    "filtered_data.set_index('date', inplace=True)\n",
    "datasets_with_labels = []\n",
    "\n",
    "grouped = filtered_data.groupby('slug')\n",
    "\n",
    "for crypto, group in grouped:\n",
    "    close_values = group['close'].values\n",
    "\n",
    "    for start in range(len(close_values) - 9):\n",
    "        end = start + 10\n",
    "        window = close_values[start:end]\n",
    "        value_day_7 = window[6]  \n",
    "        value_day_10 = window[9] \n",
    "        label = 1 if value_day_10 > value_day_7 else 0\n",
    "\n",
    "        datasets_with_labels.append((window, label))\n",
    "\n",
    "combined_table = pd.DataFrame(datasets_with_labels, columns=['close_values', 'label'])\n",
    "\n",
    "# Print to check\n",
    "print(combined_table)\n",
    "missing_values = combined_table.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    11709\n",
      "1    10262\n",
      "Name: count, dtype: int64\n",
      "Imbalance Ratio: 0.8764198479801861\n"
     ]
    }
   ],
   "source": [
    "# Checking balance of the dataset\n",
    "print(combined_table['label'].value_counts())\n",
    "combined_table['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "class_counts = combined_table['label'].value_counts()\n",
    "imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "X = combined_table['close_values'].apply(lambda x: x[:7]).tolist()\n",
    "y = combined_table['label'].astype(int).tolist()\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"Multi-Layer Perceptron\": MLPClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Balanced Random Forest\": BalancedRandomForestClassifier(replacement=True, sampling_strategy='all', random_state=42, bootstrap=False),\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "results = {clf_name: {'balanced_accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': [], 'log_loss': []} for clf_name in classifiers}\n",
    "\n",
    "# Evaluation loop\n",
    "for clf_name, clf in classifiers.items():\n",
    "    for train, test in rskf.split(X, y):\n",
    "        model = clf\n",
    "        model.fit(X[train], y[train])\n",
    "        y_pred = model.predict(X[test])\n",
    "        y_prob = model.predict_proba(X[test]) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        results[clf_name]['balanced_accuracy'].append(balanced_accuracy_score(y[test], y_pred))\n",
    "        results[clf_name]['precision'].append(precision_score(y[test], y_pred))\n",
    "        results[clf_name]['recall'].append(recall_score(y[test], y_pred))\n",
    "        results[clf_name]['f1'].append(f1_score(y[test], y_pred))\n",
    "        if y_prob is not None:\n",
    "            results[clf_name]['roc_auc'].append(roc_auc_score(y[test], y_prob[:, 1]))\n",
    "            results[clf_name]['log_loss'].append(log_loss(y[test], y_prob))\n",
    "        else:\n",
    "            # For classifiers without predict_proba, handle ROC AUC and Log Loss differently\n",
    "            results[clf_name]['roc_auc'].append(np.nan)\n",
    "            results[clf_name]['log_loss'].append(np.nan)\n",
    "\n",
    "\n",
    "print(\"Mean Performance Metrics:\")\n",
    "for metric in ['balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'log_loss']:\n",
    "    print(f\"{metric.capitalize()}:\")\n",
    "    for clf_name, scores in results.items():\n",
    "        mean_score = np.nanmean(scores[metric])  \n",
    "        std_score = np.nanstd(scores[metric])    \n",
    "        print(f\"   {clf_name}: Mean = {mean_score:.4f}, Std = {std_score:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "metrics = ['balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'log_loss']\n",
    "titles = ['Balanced Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Log Loss']\n",
    "\n",
    "for ax, metric, title in zip(axs.ravel(), metrics, titles):\n",
    "    ax.boxplot([results[clf][metric] for clf in classifiers], labels=classifiers.keys())\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Classifiers')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_xticklabels(classifiers.keys(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "for clf_name, metrics in results.items():\n",
    "    print(f\"{clf_name} Performance Metrics:\")\n",
    "    for metric, scores in metrics.items():\n",
    "        mean_score = np.nanmean(scores)  # Use nanmean to handle NaN values for some metrics\n",
    "        std_score = np.nanstd(scores)    # Use nanstd to handle NaN values for some metrics\n",
    "        print(f\"   {metric.capitalize()}: Mean = {mean_score:.4f}, Std = {std_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
